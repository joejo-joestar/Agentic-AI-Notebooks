{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5200c8a",
   "metadata": {},
   "source": [
    "# 01 - Multi Agent Orchestration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d08c6",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "These are the required _Libraries_ and _Environment Variables_ for this notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875e052",
   "metadata": {},
   "source": [
    "### Libraries Required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca954bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for setting up Jupyter widgets and notebook features\n",
    "%conda install conda-forge::ipywidgets --update-deps --force-reinstall\n",
    "%conda install conda-forge::ipykernel --update-deps --force-reinstall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09feed",
   "metadata": {},
   "source": [
    "- [Anthropic](https://docs.claude.com/en/docs/get-started#python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install conda-forge::anthropic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b994f64",
   "metadata": {},
   "source": [
    "### Variables Required\n",
    "\n",
    "| Token Name        | `.env` Name         | Where to Get / Setting Value                                     |                                                     Reference |\n",
    "| :---------------- | :------------------ | :--------------------------------------------------------------- | ------------------------------------------------------------: |\n",
    "| Anthropic API Key | `ANTHROPIC_API_KEY` | [Anthropic Console](https://console.anthropic.com/settings/keys) | [Anthropic API Docs](https://docs.claude.com/en/api/overview) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c13ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT = Path().resolve().parent.parent\n",
    "sys.path.append(str(ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    set_env_variables,\n",
    ")\n",
    "\n",
    "ENV_FILE = ROOT / \".env\"\n",
    "\n",
    "set_env_variables(env_file=ENV_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827ed8e",
   "metadata": {},
   "source": [
    "## Actual Shenanigans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict\n",
    "from enum import Enum\n",
    "import time\n",
    "\n",
    "from anthropic import Anthropic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626d69c",
   "metadata": {},
   "source": [
    "### Defining Base Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895745c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Severity(Enum):\n",
    "    CRITICAL = 4\n",
    "    HIGH = 3\n",
    "    MEDIUM = 2\n",
    "    LOW = 1\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Issue:\n",
    "    \"\"\"Represents a code issue found by an agent\"\"\"\n",
    "\n",
    "    issue_type: str\n",
    "    severity: Severity\n",
    "    line_number: int\n",
    "    description: str\n",
    "    suggested_fix: str\n",
    "    agent_name: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CodeReviewResult:\n",
    "    \"\"\"Aggregated results from all agents\"\"\"\n",
    "\n",
    "    issues: List[Issue] = field(default_factory=list)\n",
    "    test_cases: List[str] = field(default_factory=list)\n",
    "    summary: str = \"\"\n",
    "    total_lines_reviewed: int = 0\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Base class for all review agents\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, role: str):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.tasks_completed = 0\n",
    "        self.issues_found = []\n",
    "\n",
    "    def analyze(self, code: str) -> List[Issue]:\n",
    "        \"\"\"Override this method in subclasses\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def log_activity(self, message: str):\n",
    "        \"\"\"Log agent activity\"\"\"\n",
    "        print(f\"[{self.name}] {message}\")\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Return agent statistics\"\"\"\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"role\": self.role,\n",
    "            \"tasks_completed\": self.tasks_completed,\n",
    "            \"issues_found\": len(self.issues_found),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da70ddb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c2c13",
   "metadata": {},
   "source": [
    "### Initializing Anthropic Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53067210",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Anthropic()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dbbc30",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f070f",
   "metadata": {},
   "source": [
    "### LLM Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(prompt: str, model: str = \"claude-sonnet-4-5-20250929\") -> str:\n",
    "    \"\"\"Helper function to query Anthropic Claude API\"\"\"\n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=1500,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            system=\"You are an expert code reviewer. Provide concise, structured responses in JSON format where possible, focusing on code quality, security, and best practices.\",\n",
    "        )\n",
    "        return message.content[0].text.strip()  # type: ignore\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Query Error: {e}\")\n",
    "        return json.dumps({\"issues\": [], \"error\": str(e)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b9c3e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb5afb",
   "metadata": {},
   "source": [
    "### Agent Definitions\n",
    "\n",
    "**Agents**:\n",
    "\n",
    "- Security Reviewer\n",
    "- Bug Detector\n",
    "- Performance Analyzer\n",
    "- Style Checker\n",
    "- Test Generator\n",
    "\n",
    "**Delegation Strategy**:\n",
    "- **Task-based**: Each agent has specialized responsibility (Security ‚Üí Bug ‚Üí Performance ‚Üí Style)\n",
    "- **Priority-based**: Critical security issues halt workflow\n",
    "- **Failure handling**: Graceful degradation on LLM errors\n",
    "- **Aggregation**: Orchestrator collects, prioritizes, and generates tests\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "‚ÑπÔ∏è **Note**\n",
    "\n",
    "You can find the agent architecture in the separate [readme file](./readme.md)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857840da",
   "metadata": {},
   "source": [
    "#### Security Reviewer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ca88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecurityReviewerAgent(Agent):\n",
    "    \"\"\"Detects security vulnerabilities using Claude\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"SecurityReviewer\", \"Security Analysis\")\n",
    "\n",
    "    def analyze(self, code: str) -> List[Issue]:\n",
    "        self.log_activity(\"Querying Claude for security analysis...\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Analyze this Python code for security vulnerabilities (SQL injection, XSS, eval/exec usage, unsafe deserialization, hard-coded credentials, authentication/authorization flaws, etc.):\n",
    "\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Respond ONLY in valid JSON format (no markdown, no extra text):\n",
    "{{\"issues\": [{{\"line\": 1, \"description\": \"example\", \"severity\": \"CRITICAL\", \"fix\": \"example fix\"}}, ...]}}\n",
    "\n",
    "If no issues found, return {{\"issues\": []}}.\n",
    "Focus on real security risks in a 500-2000 line codebase.\n",
    "Prioritize CRITICAL and HIGH severity vulnerabilities.\n",
    "\"\"\"\n",
    "\n",
    "        response = query_llm(prompt)\n",
    "        issues = []\n",
    "\n",
    "        try:\n",
    "            # Try to parse JSON response\n",
    "            data = json.loads(response)\n",
    "            for issue_data in data.get(\"issues\", []):\n",
    "                severity_map = {\n",
    "                    \"CRITICAL\": Severity.CRITICAL,\n",
    "                    \"HIGH\": Severity.HIGH,\n",
    "                    \"MEDIUM\": Severity.MEDIUM,\n",
    "                    \"LOW\": Severity.LOW,\n",
    "                }\n",
    "                issue = Issue(\n",
    "                    issue_type=\"security\",\n",
    "                    severity=severity_map.get(\n",
    "                        issue_data.get(\"severity\", \"MEDIUM\"), Severity.MEDIUM\n",
    "                    ),\n",
    "                    line_number=issue_data.get(\"line\", 1),\n",
    "                    description=issue_data.get(\n",
    "                        \"description\", \"Security issue detected\"\n",
    "                    ),\n",
    "                    suggested_fix=issue_data.get(\n",
    "                        \"fix\", \"Review and apply security best practices\"\n",
    "                    ),\n",
    "                    agent_name=self.name,\n",
    "                )\n",
    "                issues.append(issue)\n",
    "        except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "            self.log_activity(f\"Failed to parse response: {e}\")\n",
    "\n",
    "        self.tasks_completed += 1\n",
    "        self.issues_found.extend(issues)\n",
    "        self.log_activity(f\"Claude found {len(issues)} security issues\")\n",
    "        return issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26149290",
   "metadata": {},
   "source": [
    "#### Bug Detector Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a937995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugDetectorAgent(Agent):\n",
    "    \"\"\"Identifies bugs using Claude\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"BugDetector\", \"Bug Detection\")\n",
    "\n",
    "    def analyze(self, code: str) -> List[Issue]:\n",
    "        self.log_activity(\"Querying Claude for bug detection...\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Identify logical, runtime, and potential bugs in this Python code (None/null checks, exception handling, type mismatches, off-by-one errors, uninitialized variables, infinite loops, etc.):\n",
    "\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Respond ONLY in valid JSON format (no markdown, no extra text):\n",
    "{{\"issues\": [{{\"line\": 1, \"description\": \"example\", \"severity\": \"HIGH\", \"fix\": \"example fix\"}}, ...]}}\n",
    "\n",
    "If no bugs found, return {{\"issues\": []}}.\n",
    "Consider edge cases and common Python pitfalls in a 500-2000 line codebase.\n",
    "\"\"\"\n",
    "\n",
    "        response = query_llm(prompt)\n",
    "        issues = []\n",
    "\n",
    "        try:\n",
    "            data = json.loads(response)\n",
    "            for issue_data in data.get(\"issues\", []):\n",
    "                severity_map = {\n",
    "                    \"CRITICAL\": Severity.CRITICAL,\n",
    "                    \"HIGH\": Severity.HIGH,\n",
    "                    \"MEDIUM\": Severity.MEDIUM,\n",
    "                    \"LOW\": Severity.LOW,\n",
    "                }\n",
    "                issue = Issue(\n",
    "                    issue_type=\"bug\",\n",
    "                    severity=severity_map.get(\n",
    "                        issue_data.get(\"severity\", \"MEDIUM\"), Severity.MEDIUM\n",
    "                    ),\n",
    "                    line_number=issue_data.get(\"line\", 1),\n",
    "                    description=issue_data.get(\"description\", \"Bug detected\"),\n",
    "                    suggested_fix=issue_data.get(\"fix\", \"Review and fix the bug\"),\n",
    "                    agent_name=self.name,\n",
    "                )\n",
    "                issues.append(issue)\n",
    "        except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "            self.log_activity(f\"Failed to parse response: {e}\")\n",
    "\n",
    "        self.tasks_completed += 1\n",
    "        self.issues_found.extend(issues)\n",
    "        self.log_activity(f\"Claude found {len(issues)} bugs\")\n",
    "        return issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b250bda",
   "metadata": {},
   "source": [
    "#### Performance Analyzer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27896513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceAnalyzerAgent(Agent):\n",
    "    \"\"\"Identifies performance issues using Claude\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"PerformanceAnalyzer\", \"Performance Analysis\")\n",
    "\n",
    "    def analyze(self, code: str) -> List[Issue]:\n",
    "        self.log_activity(\"Querying Claude for performance analysis...\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Detect performance bottlenecks in this Python code (O(n¬≤) loops, inefficient data structures, redundant computations, memory leaks, N+1 queries, unnecessary copies, etc.):\n",
    "\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Respond ONLY in valid JSON format (no markdown, no extra text):\n",
    "{{\"issues\": [{{\"line\": 1, \"description\": \"example\", \"severity\": \"MEDIUM\", \"fix\": \"example fix\"}}, ...]}}\n",
    "\n",
    "If no performance issues found, return {{\"issues\": []}}.\n",
    "Suggest optimizations with time/space complexity improvements for a 500-2000 line codebase.\n",
    "\"\"\"\n",
    "\n",
    "        response = query_llm(prompt)\n",
    "        issues = []\n",
    "\n",
    "        try:\n",
    "            data = json.loads(response)\n",
    "            for issue_data in data.get(\"issues\", []):\n",
    "                severity_map = {\n",
    "                    \"CRITICAL\": Severity.CRITICAL,\n",
    "                    \"HIGH\": Severity.HIGH,\n",
    "                    \"MEDIUM\": Severity.MEDIUM,\n",
    "                    \"LOW\": Severity.LOW,\n",
    "                }\n",
    "                issue = Issue(\n",
    "                    issue_type=\"performance\",\n",
    "                    severity=severity_map.get(\n",
    "                        issue_data.get(\"severity\", \"MEDIUM\"), Severity.MEDIUM\n",
    "                    ),\n",
    "                    line_number=issue_data.get(\"line\", 1),\n",
    "                    description=issue_data.get(\n",
    "                        \"description\", \"Performance issue detected\"\n",
    "                    ),\n",
    "                    suggested_fix=issue_data.get(\"fix\", \"Implement optimization\"),\n",
    "                    agent_name=self.name,\n",
    "                )\n",
    "                issues.append(issue)\n",
    "        except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "            self.log_activity(f\"Failed to parse response: {e}\")\n",
    "\n",
    "        self.tasks_completed += 1\n",
    "        self.issues_found.extend(issues)\n",
    "        self.log_activity(f\"Claude found {len(issues)} performance issues\")\n",
    "        return issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a586a2",
   "metadata": {},
   "source": [
    "#### Style Checker Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleCheckerAgent(Agent):\n",
    "    \"\"\"Validates code style using Claude\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"StyleChecker\", \"Code Style Validation\")\n",
    "\n",
    "    def analyze(self, code: str) -> List[Issue]:\n",
    "        self.log_activity(\"Querying Claude for style check...\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Validate code style against PEP 8, naming conventions, docstrings, line length, and professional standards in this Python code:\n",
    "\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Respond ONLY in valid JSON format (no markdown, no extra text):\n",
    "{{\"issues\": [{{\"line\": 1, \"description\": \"example\", \"severity\": \"LOW\", \"fix\": \"example fix\"}}, ...]}}\n",
    "\n",
    "If no style issues found, return {{\"issues\": []}}.\n",
    "Suggest improvements for maintainability and readability in a 500-2000 line codebase.\n",
    "\"\"\"\n",
    "\n",
    "        response = query_llm(prompt)\n",
    "        issues = []\n",
    "\n",
    "        try:\n",
    "            data = json.loads(response)\n",
    "            for issue_data in data.get(\"issues\", []):\n",
    "                severity_map = {\n",
    "                    \"CRITICAL\": Severity.CRITICAL,\n",
    "                    \"HIGH\": Severity.HIGH,\n",
    "                    \"MEDIUM\": Severity.MEDIUM,\n",
    "                    \"LOW\": Severity.LOW,\n",
    "                }\n",
    "                issue = Issue(\n",
    "                    issue_type=\"style\",\n",
    "                    severity=severity_map.get(\n",
    "                        issue_data.get(\"severity\", \"LOW\"), Severity.LOW\n",
    "                    ),\n",
    "                    line_number=issue_data.get(\"line\", 1),\n",
    "                    description=issue_data.get(\"description\", \"Style issue detected\"),\n",
    "                    suggested_fix=issue_data.get(\"fix\", \"Apply style fix\"),\n",
    "                    agent_name=self.name,\n",
    "                )\n",
    "                issues.append(issue)\n",
    "        except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "            self.log_activity(f\"Failed to parse response: {e}\")\n",
    "\n",
    "        self.tasks_completed += 1\n",
    "        self.issues_found.extend(issues)\n",
    "        self.log_activity(f\"Claude found {len(issues)} style issues\")\n",
    "        return issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4880c2b",
   "metadata": {},
   "source": [
    "#### Test Generator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestGeneratorAgent(Agent):\n",
    "    \"\"\"Generates test cases using Claude\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"TestGenerator\", \"Test Case Generation\")\n",
    "\n",
    "    def generate_tests(self, code: str, issues: List[Issue]) -> List[str]:\n",
    "        self.log_activity(\"Querying Claude for test generation...\")\n",
    "\n",
    "        issues_str = \"\\n\".join(\n",
    "            [\n",
    "                f\"Line {i.line_number}: {i.description} ({i.severity.name})\"\n",
    "                for i in issues[:10]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Generate comprehensive test cases (unit, integration, edge cases, error handling) for this Python code.\n",
    "\n",
    "Focus on covering these issues:\n",
    "{issues_str}\n",
    "\n",
    "Code to test:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Provide complete, runnable Python test functions using pytest style. Each test should have descriptive names and cover normal cases, edge cases, and error scenarios.\n",
    "Include necessary imports (pytest, unittest, mocking, etc.).\n",
    "\"\"\"\n",
    "\n",
    "        response = query_llm(prompt)\n",
    "        test_cases = response.split(\"def test_\")[1:]\n",
    "        test_cases = [f\"def test_{tc.strip()}\" for tc in test_cases if tc.strip()]\n",
    "\n",
    "        self.tasks_completed += 1\n",
    "        self.log_activity(f\"‚úÖ Claude generated {len(test_cases)} test cases\")\n",
    "        return test_cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1f48d",
   "metadata": {},
   "source": [
    "#### Orchestration Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba639008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestratorAgent(Agent):\n",
    "    \"\"\"\n",
    "    Coordinates the code review workflow using intelligent delegation.\n",
    "\n",
    "    Delegation Strategy:\n",
    "    1. Task-based delegation: Route to specialized agents based on task type\n",
    "    2. Priority-based execution: Security ‚Üí Bugs ‚Üí Performance ‚Üí Style\n",
    "    3. Parallel processing simulation: Independent agents work concurrently\n",
    "    4. Aggregation: Collect and prioritize all findings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Orchestrator\", \"Workflow Coordination\")\n",
    "\n",
    "        # Initialize specialized agents\n",
    "        self.security_agent = SecurityReviewerAgent()\n",
    "        self.bug_agent = BugDetectorAgent()\n",
    "        self.performance_agent = PerformanceAnalyzerAgent()\n",
    "        self.style_agent = StyleCheckerAgent()\n",
    "        self.test_agent = TestGeneratorAgent()\n",
    "\n",
    "        self.agents = [\n",
    "            self.security_agent,\n",
    "            self.bug_agent,\n",
    "            self.performance_agent,\n",
    "            self.style_agent,\n",
    "            self.test_agent,\n",
    "        ]\n",
    "\n",
    "        self.delegation_log = []\n",
    "\n",
    "    def delegate_task(self, agent: Agent, task_type: str, code: str) -> List[Issue]:\n",
    "        \"\"\"Delegate specific task to an agent\"\"\"\n",
    "        self.log_activity(f\"üì§ Delegating '{task_type}' to {agent.name}\")\n",
    "        self.delegation_log.append(\n",
    "            {\n",
    "                \"from\": self.name,\n",
    "                \"to\": agent.name,\n",
    "                \"task\": task_type,\n",
    "                \"timestamp\": time.time(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return agent.analyze(code)\n",
    "\n",
    "    def orchestrate_review(self, code: str) -> CodeReviewResult:\n",
    "        \"\"\"\n",
    "        Main orchestration method implementing the delegation strategy\n",
    "        \"\"\"\n",
    "        self.log_activity(\"Starting code review workflow...\")\n",
    "        self.log_activity(f\"Code size: {len(code.splitlines())} lines\")\n",
    "\n",
    "        result = CodeReviewResult(total_lines_reviewed=len(code.splitlines()))\n",
    "\n",
    "        # Phase 1: Security (highest priority)\n",
    "        self.log_activity(\"\\n=== PHASE 1: Security Review ===\")\n",
    "        security_issues = self.delegate_task(\n",
    "            self.security_agent, \"Security Analysis\", code\n",
    "        )\n",
    "        result.issues.extend(security_issues)\n",
    "\n",
    "        # Failure Mode 1: Critical security issues halt further review\n",
    "        critical_security = [\n",
    "            i for i in security_issues if i.severity == Severity.CRITICAL\n",
    "        ]\n",
    "        if critical_security:\n",
    "            self.log_activity(\n",
    "                f\"FAILURE MODE TRIGGERED: {len(critical_security)} critical security issues found!\"\n",
    "            )\n",
    "            self.log_activity(\"Halting review - security issues must be fixed first\")\n",
    "            result.summary = f\"BLOCKED: {len(critical_security)} critical security vulnerabilities must be addressed before proceeding.\"\n",
    "            return result\n",
    "\n",
    "        # Phase 2: Bug Detection\n",
    "        self.log_activity(\"\\n=== PHASE 2: Bug Detection ===\")\n",
    "        bug_issues = self.delegate_task(self.bug_agent, \"Bug Detection\", code)\n",
    "        result.issues.extend(bug_issues)\n",
    "\n",
    "        # Phase 3: Performance Analysis\n",
    "        self.log_activity(\"\\n=== PHASE 3: Performance Analysis ===\")\n",
    "        perf_issues = self.delegate_task(\n",
    "            self.performance_agent, \"Performance Analysis\", code\n",
    "        )\n",
    "        result.issues.extend(perf_issues)\n",
    "\n",
    "        # Phase 4: Style Check\n",
    "        self.log_activity(\"\\n=== PHASE 4: Style Validation ===\")\n",
    "        style_issues = self.delegate_task(self.style_agent, \"Style Check\", code)\n",
    "        result.issues.extend(style_issues)\n",
    "\n",
    "        # Phase 5: Test Generation\n",
    "        self.log_activity(\"\\n=== PHASE 5: Test Case Generation ===\")\n",
    "        result.test_cases = self.test_agent.generate_tests(code, result.issues)\n",
    "\n",
    "        # Failure Mode 2: No issues found (possible false negative)\n",
    "        if len(result.issues) == 0:\n",
    "            self.log_activity(\"FAILURE MODE TRIGGERED: No issues detected!\")\n",
    "            self.log_activity(\n",
    "                \"Recommendation: Run with stricter rule set or manual review\"\n",
    "            )\n",
    "            result.summary = \"No automated issues found. Consider manual peer review for complex logic.\"\n",
    "        else:\n",
    "            # Prioritize issues by severity\n",
    "            result.issues.sort(key=lambda x: x.severity.value, reverse=True)\n",
    "            result.summary = self.generate_summary(result.issues)\n",
    "\n",
    "        self.log_activity(\"\\nCode review workflow completed!\")\n",
    "        return result\n",
    "\n",
    "    def generate_summary(self, issues: List[Issue]) -> str:\n",
    "        \"\"\"Generate executive summary of findings\"\"\"\n",
    "        severity_counts = {\n",
    "            \"CRITICAL\": len([i for i in issues if i.severity == Severity.CRITICAL]),\n",
    "            \"HIGH\": len([i for i in issues if i.severity == Severity.HIGH]),\n",
    "            \"MEDIUM\": len([i for i in issues if i.severity == Severity.MEDIUM]),\n",
    "            \"LOW\": len([i for i in issues if i.severity == Severity.LOW]),\n",
    "        }\n",
    "\n",
    "        summary = f\"Found {len(issues)} total issues: \"\n",
    "        summary += f\"{severity_counts['CRITICAL']} critical, \"\n",
    "        summary += f\"{severity_counts['HIGH']} high, \"\n",
    "        summary += f\"{severity_counts['MEDIUM']} medium, \"\n",
    "        summary += f\"{severity_counts['LOW']} low severity.\"\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def get_delegation_stats(self) -> Dict:\n",
    "        \"\"\"Return delegation statistics\"\"\"\n",
    "        return {\n",
    "            \"total_delegations\": len(self.delegation_log),\n",
    "            \"agents_used\": len(self.agents),\n",
    "            \"delegation_log\": self.delegation_log,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bdf601",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0debb41",
   "metadata": {},
   "source": [
    "### Display Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(result: CodeReviewResult):\n",
    "    \"\"\"Display formatted review results\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Code Review Results\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(f\"\\nSummary: {result.summary}\")\n",
    "    print(f\"  Lines Reviewed: {result.total_lines_reviewed}\")\n",
    "    print(f\"  Total Issues: {len(result.issues)}\")\n",
    "    print(f\"  Test Cases Generated: {len(result.test_cases)}\")\n",
    "\n",
    "    if result.issues:\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"Issues by Severity\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        for severity in [\n",
    "            Severity.CRITICAL,\n",
    "            Severity.HIGH,\n",
    "            Severity.MEDIUM,\n",
    "            Severity.LOW,\n",
    "        ]:\n",
    "            severity_issues = [i for i in result.issues if i.severity == severity]\n",
    "            if severity_issues:\n",
    "                print(f\"\\n{severity.name} ({len(severity_issues)} issues):\")\n",
    "                for issue in severity_issues:\n",
    "                    print(f\"\\n  Line {issue.line_number} | {issue.issue_type.upper()}\")\n",
    "                    print(f\"  Agent: {issue.agent_name}\")\n",
    "                    print(f\"  Issue: {issue.description}\")\n",
    "                    print(f\"  Fix: {issue.suggested_fix}\")\n",
    "\n",
    "    if result.test_cases:\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"Generated Test Cases\")\n",
    "        print(\"-\" * 70)\n",
    "        for i, test in enumerate(result.test_cases[:3], 1):\n",
    "            print(f\"\\nTest {i}:\")\n",
    "            print(test[:200] + (\"...\" if len(test) > 200 else \"\"))\n",
    "        if len(result.test_cases) > 3:\n",
    "            print(f\"\\n... and {len(result.test_cases) - 3} more tests\")\n",
    "\n",
    "\n",
    "def display_agent_stats(orchestrator: OrchestratorAgent):\n",
    "    \"\"\"Display agent performance statistics\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Agent Performance Statistics\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for agent in orchestrator.agents:\n",
    "        stats = agent.get_stats()\n",
    "        print(f\"\\n{stats['name']}:\")\n",
    "        print(f\"  Role: {stats['role']}\")\n",
    "        print(f\"  Tasks Completed: {stats['tasks_completed']}\")\n",
    "        print(f\"  Issues Found: {stats['issues_found']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb194d21",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8892f",
   "metadata": {},
   "source": [
    "### Running the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b82cc",
   "metadata": {},
   "source": [
    "#### Demo the agent with example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2233f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_CODE = '''\n",
    "def process_user_data(user_input, db_connection):\n",
    "    \"\"\"Process user data and store in database.\"\"\"\n",
    "    query = \"SELECT * FROM users WHERE id = \" + user_input\n",
    "    result = db_connection.execute(query)\n",
    "\n",
    "    data = \"\"\n",
    "    for item in result:\n",
    "        data += str(item)  # Performance issue\n",
    "\n",
    "    return data\n",
    "\n",
    "def calculate_value(x, y):\n",
    "    \"\"\"Calculate value from inputs.\"\"\"\n",
    "    if x == None:  # Bug: should use 'is None'\n",
    "        return 0\n",
    "    try:\n",
    "        value = eval(\"x + y\")  # Security: dangerous eval\n",
    "    except:  # Bug: bare except\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "def validate_and_process(data):\n",
    "    \"\"\"Validate and process data.\"\"\"\n",
    "    if data == True:  # Bug: redundant comparison\n",
    "        return process_user_data(data, None)\n",
    "    return None\n",
    "\n",
    "def fetch_user_info(user_id):\n",
    "    \"\"\"Fetch user information.\"\"\"\n",
    "    api_key = \"sk-1234567890abcdef\"  # Security: hard-coded credentials\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    import subprocess\n",
    "    result = subprocess.call(\"curl https://api.example.com/user/\" + user_id)  # Security: command injection\n",
    "    return result\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412774f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Demo\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "orchestrator = OrchestratorAgent()\n",
    "\n",
    "result = orchestrator.orchestrate_review(SAMPLE_CODE)\n",
    "display_results(result)\n",
    "display_agent_stats(orchestrator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e86511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Delegation Statistics\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "delegation_stats = orchestrator.get_delegation_stats()\n",
    "print(f\"\\nTotal Delegations: {delegation_stats['total_delegations']}\")\n",
    "print(f\"Agents Utilized: {delegation_stats['agents_used']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7dd5c8",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
