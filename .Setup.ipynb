{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a644b9",
   "metadata": {},
   "source": [
    "# Setting Up a Local Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41176ad6",
   "metadata": {},
   "source": [
    "> ℹ️ **Note**\n",
    ">\n",
    "> Make sure the conda environment created from the [readme](README.md) is selected as the kernel for this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad259ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ba554",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea3378",
   "metadata": {},
   "source": [
    "> ℹ️ **Note**\n",
    ">\n",
    "> It is [_recommended_](https://docs.conda.io/projects/conda/en/stable/user-guide/tasks/manage-environments.html#using-pip-in-an-environment) to look for any required package in the [conda-forge repository](https://conda-forge.org/packages/)\n",
    "> \n",
    "> To install packages from pip in the current conda environment\n",
    ">\n",
    "> Read more info in this over at the [conda documentation](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#using-pip-in-an-environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df074050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for setting up Jupyter widgets and notebook features\n",
    "%conda install conda-forge::ipywidgets --update-deps --force-reinstall\n",
    "%conda install conda-forge::ipykernel --update-deps --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install pip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c645e4d",
   "metadata": {},
   "source": [
    "### Main Dependencies\n",
    "\n",
    "This is a list of dependencies you need for this notebook\n",
    "\n",
    "- [Transformers](https://huggingface.co/docs/transformers/en/index)\n",
    "- [Datasets](https://huggingface.co/docs/datasets/index)\n",
    "- [Tokenizers](https://huggingface.co/docs/tokenizers/index)\n",
    "- [Hugging Face Hub](https://huggingface.co/docs/hub/en/index)\n",
    "- [PyTorch](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install conda-forge::transformers\n",
    "%conda install conda-forge::datasets\n",
    "%conda install conda-forge::tokenizers\n",
    "%conda install conda-forge::huggingface_hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e27895",
   "metadata": {},
   "source": [
    "> ℹ️ **Note**\n",
    ">\n",
    "> Get the latest command from the [get started page](https://pytorch.org/get-started/locally/) (select based on your system configuration)\n",
    ">\n",
    "> The current script installs _**PyTorch with CUDA 12.6**_ Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a23a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA 12.6 support\n",
    "%pip install torch --index-url https://download.pytorch.org/whl/cu126\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b589b6",
   "metadata": {},
   "source": [
    "### External APIs and Connectors\n",
    "\n",
    "- [Gemini API](https://ai.google.dev/gemini-api/docs/quickstart)\n",
    "- [OpenAI API](https://platform.openai.com/docs/api-reference/introduction)\n",
    "- [LangSmith](https://docs.langchain.com/langsmith/home)\n",
    "- [LangChain](https://reference.langchain.com/python/langchain/langchain/)\n",
    "- [LangChain Core](https://reference.langchain.com/python/langchain_core/)\n",
    "- [LangChain Google genai](https://docs.langchain.com/oss/python/integrations/providers/google) integration package (Requires `GOOGLE_API_KEY` to be set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Model APIs\n",
    "%conda install conda-forge::google-genai\n",
    "%conda install conda-forge::openai\n",
    "%conda install conda-forge::langsmith\n",
    "%conda install conda-forge::langchain\n",
    "%conda install conda-forge::langchain-core\n",
    "\n",
    "# for LangChain Google Gemini Integration\n",
    "%conda install conda-forge::langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1843c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584c6839",
   "metadata": {},
   "source": [
    "## Setting Environment Variables\n",
    "\n",
    "A lot of the models and frameworks used in the notebooks in this repo require a Private Access Token or some sort of Secret Keys.\n",
    "To make loading them easier, follow these steps:\n",
    "\n",
    "Before running the following codeblock, create a `.env` file following the [example file](.env.example) provided.\n",
    "\n",
    "The script can be found over in [`utils/env_variables_setup.py`](utils/env_variables_setup.py).\n",
    "\n",
    "More details of this script will can be found in the [readme](README.md)\n",
    "\n",
    "> ℹ️ **Note**\n",
    ">\n",
    "> This script only has to be run once when setting up the environment. Conda environments store the set keys in the environment files (which means they will persist even after restarting the kernel).\n",
    ">\n",
    "> Anytime you update an entry in the `.env` file, you have to run the script to add/update the new/updated variable to the conda environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b780cd97",
   "metadata": {},
   "source": [
    "### Variables Required in This Notebook\n",
    "\n",
    "| Token Name                     | `.env` Name          | Where to Get / Setting Value                                                      |                                                                                                                                                                        Reference |\n",
    "| :----------------------------- | :------------------- | :-------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |\n",
    "| Hugging Face User Access Token | `HF_TOKEN`           | [Hugging Face Settings](https://huggingface.co/settings/tokens)                   |                                                                                                                                 [Hugging Face Docs](https://huggingface.co/docs) |\n",
    "| Google Gemini API Key          | `GOOGLE_API_KEY`     | [Google AI Studio](https://aistudio.google.com/api-keys)                          | [Gemini API Docs](https://ai.google.dev/gemini-api/docs/quickstart) and [LangChain Google Integration Docs](https://docs.langchain.com/oss/python/integrations/providers/google) |\n",
    "| OpenAI API Key                 | `OPENAI_API_KEY`     | [OpenAI API Platform](https://platform.openai.com/settings/organization/api-keys) |                                                                                              [OpenAI API Reference](https://platform.openai.com/docs/api-reference/introduction) |\n",
    "| LangSmith API Key              | `LANGSMITH_API_KEY`  | [LangSmith Settings](https://smith.langchain.com/settings)                        |                                                                                                             [LangSmith API Reference](https://docs.langchain.com/langsmith/home) |\n",
    "| LangSmith Tracing              | `LANGSMITH_TRACING`  | A Boolean, set the value to `true` or `false` to enable or disable logging traces |                                                                           [LangSmith Observability API Reference](https://docs.langchain.com/langsmith/observability-quickstart) |\n",
    "| LangSmith Endpoint             | `LANGSMITH_ENDPOINT` | The LangSmith Endpoint to log the traces (<https://api.smith.langchain.com>)      |                                                                           [LangSmith Observability API Reference](https://docs.langchain.com/langsmith/observability-quickstart) |\n",
    "| LangSmith Project Name         | `LANGSMITH_PROJECT`  | The name of the project to log the traces under                                   |                                                                           [LangSmith Observability API Reference](https://docs.langchain.com/langsmith/observability-quickstart) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d360ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path().resolve()\n",
    "ENV_FILE = ROOT / \".env\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146798fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.env_variables_setup import (\n",
    "    set_env_variables,\n",
    ")\n",
    "\n",
    "set_env_variables(env_file=ENV_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unset an environment variable in the conda env.\n",
    "\n",
    "from utils.env_variables_setup import (\n",
    "    unset_env_variable,\n",
    ")\n",
    "\n",
    "unset_env_variable(\"SOME_API_KEY\")  # replace with actual variable name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29486c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all environment variables set in the conda env.\n",
    "\n",
    "from utils.env_variables_setup import (\n",
    "    list_env_variables,\n",
    ")\n",
    "\n",
    "list_env_variables()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77fce9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fe9d7",
   "metadata": {},
   "source": [
    "## Testing the Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac26ef",
   "metadata": {},
   "source": [
    "Some simple code snippets to try out the `transformers` package\n",
    "\n",
    "> 💡**Tip**\n",
    ">\n",
    "> Feel free to go through the [Transformers documentation](https://huggingface.co/docs/transformers/en/index) for the full resource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0667ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence is transforming the world by bringing us to a new understanding of how things work, and has a positive impact on our lives.\n",
      "\n",
      "There are a few things we can do to help.\n",
      "\n",
      "1. Don't ignore the fact that we have no idea how things work.\n",
      "\n",
      "People have been saying for years that machines are better at solving problems than humans.\n",
      "\n",
      "But that doesn't make them stupid.\n",
      "\n",
      "It makes them smart.\n",
      "\n",
      "2. Don't pretend that these problems aren't real.\n",
      "\n",
      "If you can't solve them then you should pretend that they don't exist.\n",
      "\n",
      "If you can't solve them then you should believe that the problem is too big, too hard, too complicated.\n",
      "\n",
      "If you can't solve them then you should believe that the problem is too complex, so complex that it's hard for people to handle it.\n",
      "\n",
      "3. Don't let people tell you that you're crazy.\n",
      "\n",
      "Why?\n",
      "\n",
      "Because you don't want to admit that you're crazy.\n",
      "\n",
      "You do want to admit that you're nuts.\n",
      "\n",
      "And you do want to admit that you're wrong.\n",
      "\n",
      "Why?\n",
      "\n",
      "Because you're convinced that you're wrong.\n",
      "\n",
      "To deny that people are crazy is\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "prompt = \"Artificial intelligence is transforming the world by\"\n",
    "generated_text = generator(\n",
    "    prompt, max_length=50, truncation=True, num_return_sequences=1\n",
    ")\n",
    "print(generated_text[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4aea49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI', 'Ġagents', 'Ġcan', 'Ġsolve', 'Ġphysics', 'Ġproblems']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"AI agents can solve physics problems\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0dedf",
   "metadata": {},
   "source": [
    "Some code snippets to try out the `openai` package\n",
    "\n",
    "> 💡**Tip**\n",
    ">\n",
    "> Feel free to go through the [OpenAI API documentation](https://platform.openai.com/docs/api-reference/introduction) for the full resource\n",
    "\n",
    "> ℹ️ **Note**\n",
    ">\n",
    "> OpenAI requires you to add a minimum credit to use the API service!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67360001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# The client gets the API key from the environment variable `OPENAI_API_KEY`.\n",
    "client = OpenAI()\n",
    "\n",
    "try:\n",
    "    response = client.responses.create(\n",
    "        model=\"o3-mini\", input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    "    )\n",
    "    print(response.output_text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27c3b4",
   "metadata": {},
   "source": [
    "Some code snippets to try out the `google-genai` package\n",
    "\n",
    "> 💡**Tip**\n",
    ">\n",
    "> Feel free to go through the [Gemini API documentation](https://ai.google.dev/gemini-api/docs/quickstart) for the full resource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d76e7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make predictions or decisions.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f236f",
   "metadata": {},
   "source": [
    "Some code snippets to try out the `langsmith` package\n",
    "\n",
    "> 💡**Tip**\n",
    ">\n",
    "> Feel free to go through the [LangSmith documentation](https://docs.langchain.com/langsmith/home) for the full resource\n",
    "\n",
    "The following scripts are a slightly modified version from the [**Prompt Engineering**](https://docs.langchain.com/langsmith/prompt-engineering-quickstart#sdk) Section of the LangSmith Docs. The change is to use Google's Gemini Model.\n",
    "\n",
    "> ℹ️ **Note**\n",
    ">\n",
    "> The [`@traceable` decorator](https://docs.langchain.com/langsmith/annotate-code) is used to log the traces (you can check your traces in your [LangSmith Dashboard](https://smith.langchain.com/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c437b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/prompt-quickstart/a33acd1a?organizationId=92ed1633-de13-40f4-8f6d-a12df5506eef'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create_prompt\n",
    "\n",
    "from langsmith import Client\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "client = Client()\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot. You Speak fluent nonsense.\"),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "client.push_prompt(\"prompt-quickstart\", object=prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b65ef3",
   "metadata": {},
   "source": [
    "> ℹ️ **Note**\n",
    ">\n",
    "> This code uses the standalone model package ([`langchain-google-genai`](https://reference.langchain.com/python/integrations/langchain_google_genai/))\n",
    ">\n",
    "> Read the docs for [Provider Specific Integration](https://reference.langchain.com/python/integrations/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a659eb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, the sky! Its color is a particularly boingy shade of gribble-fluff, often tinged with the faint aroma of forgotten Tuesdays and the distant echo of a startled teacup. On especially whimsical days, it might even shimmer with the faint, buttery glow of a well-told secret!\n"
     ]
    }
   ],
   "source": [
    "# model specific integration\n",
    "\n",
    "from langsmith import Client, traceable\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "@traceable\n",
    "def model_specific_integration():\n",
    "    client = Client()\n",
    "\n",
    "    prompt_obj = client.pull_prompt(\"prompt-quickstart\")\n",
    "    formatted = prompt_obj.invoke({\"question\": \"What is the color of the sky?\"})\n",
    "\n",
    "    # Instantiate Google GenAI model\n",
    "    gemini_2_flash = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # Invoke the model with the formatted prompt\n",
    "    # The Google GenAI integration expects lists of messages (system/human) as documented:\n",
    "    response = gemini_2_flash.invoke(formatted.messages)\n",
    "\n",
    "    print(response.content)\n",
    "\n",
    "\n",
    "model_specific_integration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475182d7",
   "metadata": {},
   "source": [
    "> ℹ️ **Note**\n",
    ">\n",
    "> This code uses the `langchain` package to create an instance of the gemini model\n",
    ">\n",
    "> Read the docs for the [model initialization](https://reference.langchain.com/python/langchain/models/#chat-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f33b4fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, the sky! Its color is a particularly boingy shade of gribble-fluff, often tinged with the faint aroma of forgotten Tuesdays and the distant echo of a startled teacup. On especially whimsical days, it might even shimmer with the faint, buttery glow of a well-told secret!\n"
     ]
    }
   ],
   "source": [
    "# generic integration\n",
    "\n",
    "from langsmith import Client, traceable\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "@traceable\n",
    "def generic_integration():\n",
    "    client = Client()\n",
    "\n",
    "    prompt_obj = client.pull_prompt(\"prompt-quickstart\")\n",
    "    formatted = prompt_obj.invoke({\"question\": \"What is the color of the sky?\"})\n",
    "\n",
    "    # Instantiate Google GenAI model\n",
    "    gemini_2_flash = init_chat_model(\"google_genai:gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "    # Invoke the model with the formatted prompt\n",
    "    response = gemini_2_flash.invoke(formatted.messages)\n",
    "\n",
    "    print(response.content)\n",
    "\n",
    "\n",
    "generic_integration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a543a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
